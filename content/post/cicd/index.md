---
title: "关于CI/CD"
date: 2023-09-08T23:21:00+08:00
tags: [CI,CD,Jenkins,docker,kubernetes]
toc: true
---

## 写在前面

之前已经回顾过了
* 流量接入与分发（Nginx/OpenResty/Kong/Traefik）
* 监控系统（Open Falcon/Prometheus/Thanos/VictoriaMetrics
* （提到过一点的）统一运维平台（其中包括基于Ansible开发的配置管理系统、对接第三方服务的DNS管理系统、WebUI等）
* （开了一个天坑）服务运行环境 Kubernetes

我们工作拼图中剩下的一大块就是CI/CD系统了，它串联起了项目的开发、测试、部署和运维管理完成过程，相信具有一定规模的公司里，CI/CD系统都是基础设施很重要的一环。本篇文章我们就来回顾一些我们在这方面做的一些工作。

## 简单讲讲CI/CD

CI: 持续集成
CD：持续交付、持续部署

当我们谈论CI/CD的时候，我们谈论更多的是一种软件工程上的方法轮：软件开发团队之间如何高效协作完成产品的交付。

说的实际一点就是: 研发团队开发规范、团队间之间如何协调（代码的提交与集成）、测试团队如何高效完成测试、运维团队如何快速完成项目的部署、监控、维护和反馈。

以这个目标为核心，从瀑布模型到敏捷开发等等，出现了各种开发模式，以这些模型为基础又产生了各种工程实践，出现了系列的工具和系统，它们或多或少的解决了整个开发模型中某些环节的问题。

一个公司，无论处于什么阶段，都会建设自己的一套工具和系统，来实践自己开发模式。Baidu 的 Noah系统、Bytedance 的 ByteCycle（GitLab+SCM+TCE+...) 等等。

CI/CD 实际上就是工程实践中的概念，是集成与部署的最终落地形式，即我们通过什么手段完成项目的集成与部署。


{{< figureCupper
img="figure1-devops.png"
caption="Devops 最为常见的示意图 （图片来自网络）"
command="Resize"
options="1080x" >}}

Devops 的概念已经出现好几年了，很多公司都有自己的实践。有些通过自动化系统打通 RD/QA/OP 之间的环节提高效率；有些干脆模糊 RD/QA/OP 之间的界限，通过技术手段让研发人员能独立完成研发流程中的各个环节。不论是那种方式CI/CD一定是其中的环节之一。

{{< figureCupper
img="figure2-agile-devops-cicd.jpg"
caption="敏捷开发 Devops CI/CD 的关系（图片来自网络）"
command="Resize"
options="1080x" >}}

敏捷开发（Agile）、Devops、CI/CD 几个概念各不相同似乎有在说同一件事情，整个图比较能解释其中的异同，主要从关注点方面而言。

接下来要讲讲我们的做法了，当然主要是一些系统与故事。

## 我们最早的做法

我接触到的最早期的CI/CD系统入下图。

{{< figureCupper
img="figure3-cicd-system-1.png"
caption="最早期的CI/CD系统示意图"
command="Resize"
options="1080x" >}}

当时情况是大部分服务还在机房的物理机上，通过一些非常古老系统来部署运行（我们的业务是Java生态，当时使用了Tomcat、Jetty、Resin、Mule、AcitveMQ 这一类的Java Web Server、ESB、MQ等等，这些名词对于刚刚进入职场的同学而言都应该存在于历史书中。）

这个阶段我们的CI/CD非常原始，但是也有效。大概的过程是：

1. 一台物理机上部署了Jenkins，它通过插件能完成从GitLab拉取代码库并通过Maven进行构建生成Jar/War包；
2. SRE同学通过人工方式（有些操作过于复杂没有交给Jenkins完成）去机器上执行一些部署脚本完成部署动作；
3. 最后构建产生的Jar/War包在物理机上的各种系统中（Tomcat、Jetty、Resin、Mule 等）部署运行。

由于当时服务并不复杂，服务模块数量也不多，这种以Jenkins为中心辅之以自动化脚本的方式也算是能有效完成任务。

后续的转折发生在2017年，随着服务规模的不断扩大（陆陆续续扩大到有200+的服务模块），服务架构逐渐变得复杂（引入了Kong API Gateway、Docker、Kubernetes，后续还增加了阿里云环境形成了混合云架构），这套简易的CI/CD系统开始逐渐无法支撑SRE团队的业务了。最凸显的问题是，SRE人工执行部署脚本环节（这时候处理旧系统的部署的同时，还需针对Docker/Kubernetes做Image的构建和部署）变得异常耗费精力。我印象中工作最为繁重的时候，一周有一半的时间在处理服务发布。

这种情况下，大概在2017-2018年左右我们十分**被动**地**主动**完成了一次CI/CD系统的升级改造。

## 200块的做法

说被动是因为这次改造完全是形式所迫我们不得不解决痛点问题释放劳动力，不然大家都得陷入服务发布的泥潭中；说主动是因为这次改造就是SRE团队自己发起的，没有来自上边的KPI/OKR压力同时也没有人力于资源预算，也不局限于技术方案，目标就是为了实现CI/CD的自动化，解放SRE人力去干更重要的事情。

这种情况下，当时几个并不太熟悉系统开发的SRE同学，边学变干，还硬生生的改造出了一个基本自动化的CI/CD系统。虽然现在回头看这个系统极为简陋根本没有设计可言，但在当时系统实现了自动化，真真实实地解决了SRE困境，在使用上也得到了业务同学的一致好评。

那段时光是那几年中的**黄金时代**，团队同学全身心的投入，都无拘无束的。有想法你就可以放开手脚干，没有资源你自己想办法，一不小心就有意外的收获。那时我们机会每个周末都泡在公司，想方案、出设计、写代码、调试、交流... 累了就去楼下咖啡店喝杯美式...一切都那么纯粹。以至于多年以后这个CI/CD系统一直被我们戏称为200块的 系统（这个投入就是200块的咖啡钱 同时对比公司后来大力投入开发新的CI/CD系统）。

感慨了这么多，那这个系统长啥样呢，看看下边的示意图。

{{< figureCupper
img="figure4-cicd-system-2.png"
caption="200块的CI/CD系统示意图"
command="Resize"
options="1080x" >}}

变化主要在引入了Docker构建（当然不可避免的引入了镜像仓库，我们用的Harbor），既然服务都运行在容器里了为啥构建镜像不行是吧？

在服务发布一侧，我们编写了一套脚本控制Kubernetes对服务进行升级（不要惊讶，真的是一套脚本，当时我们连Kubernetes 的 API 都没有掌握，纯粹通过脚本调用 kubectl 实现的控制。当然后来随着深入学习和个人成长，控制代码被改造成了通过 go sdk 操作 Kubernetes了。）

同时我们开发了一个简陋的WebUI，让用户能自行选择需要构建的项目需要发布的项目，这里我们认准了一点就是使用简单（因为这些工作以前都是SRE来做，改造后不能让用户负担显著变大），所以最终的效果是：用户只用选则项目名称和分支点击一键构建，用户选择项目名称和镜像版本点击一键升级，所有操作就这么多！

最终项目上线后，因为简单可靠（同时确实没有增加公司成本）得到了上下一致好评。

我们开发过程中，虽然没过多的方法论，但是回头看 **build in container** 的做法不就在有意无意的向云原生的理念上靠近吗？

## 200w的做法

2019年中，随着公司业务越来约庞大，上上下下思潮涌动...到这个规模了是不是需要有专业的基础架构研发团队了？我们的系统是不是需要紧跟开源社区的趋势？CI/CD是不是要更加的流畅和智能？

这个期间公司终于还是引入了专门的基础架构研发团队（当然不只是为了做CI/CD），并开始大刀阔斧的重构CI/CD系统。

新版本的系统SRE团队自然没有太多机会参与开发，我们只能在一旁观察学习...具体的实现细节我们不得而知，最终了解到的系统架构如下图示意。

{{< figureCupper
img="figure5-cicd-system-3.png"
caption="200w的CI/CD系统示意图"
command="Resize"
options="1080x" >}}

主要的变化

* 抛弃了Jenkins，引入了自己了流程控制（据说是airflow实现的，我个人并不觉得Jenkins有啥问题，而且基于我后来做BPMN系统的开发经验，当时airflow的使用也只是做了一个固定的构建发布流程，意义有限。）；
* 引入了 ArgoCD，实现了 Kubernetes 上服务发布的自动控制，这一点还是比较有意义的。

这个过程比较一波三折，前后开发了两个版本，最终才落地下来。最大的问题就是新组件的基础架构研发团队对公司研发系统和研发同学缺乏了解。第一个版本主要以 Devops 思想指导系统开发，把各种研发同学不熟悉的内容统统暴露出来，让研发同学无所侍从，从一键是操作到要去学习各种知识概念，过程也变得复杂，所以系统落地不下来。后来换了一波人又重新写了一边系统，简化了流程，同时加大了对研发同学的培训，才使得系统被慢慢接受。

这个版本的CI/CD系统前前后后的折腾，加上人力和资源投入，对比最终实现的效果...一致于我们后来总是戏谑这是200w的系统。

当然，该说不说，这个版本向云原生方向又迈进了一步。

## 当前的一些做法

后来的故事随着大家的逐渐离开，变得不得而知了。

这一部分的系统示意图是我个人学习中总结的。只能说个大概，毕竟都是纸上功夫。

{{< figureCupper
img="figure6-cicd-system-4.png"
caption="当前常见的CI/CD系统示意图"
command="Resize"
options="1080x" >}}

目前 GitOps 方法实践下的CI/CD系统可能是这样的：

* 用户向Git仓库commit代码，触发构建；
* 构建过程使用 Tekton 来控制，这是现在比较受欢迎的工具了，构建在容器中完成，生成镜像推送的Harbor中，同时使用 Helm 来生成或更新服务应用定义的Chart文件，并推送到应用的Git仓库中；
* Chart 仓库的更新触发 ArgoCD 来发起 Kubernetes 上的服务升级过程；
* 最终 Kubernetes 完成服务的升级。

我总结的这么做的一些优点

* GitOps 思想指导下操作都是通过Git来触发的：有版本控制做历史记录，方便追溯和回滚；CI和CD相关Git仓库分离更好的控制研发和SRE职责范围，影响可控；
* Tekton 实现了 Cloud Native CI；
* ArgoCD 实现了 Coud Native CD；
* 这个过程无过多的人工干预，自动化程度非常高，只有研发提交代码、前期SRE对各个环节的配置、发布过程中的监控观察。

后续还有机会的话，希望能在某处实践一下这个方案。

## 一些感悟

至此，关于CI/CD的故事基本讲完了，说一些我的感悟吧。

* 自驱的学习是快乐的！
* 只要实事求是，200块的项目能解决问题不一定就比200w的项目差😄
* 有些事情可以自上而下的推也可以自下而上的推，关键看当下的时机与处境。
* 做完工作多总结一些，提炼出来的观点就是你的方法论，就像在CI/CD系统开发过程中，我们看到了CloudNative的影子；包括**罗师傅** 想了一个通过 git 来控制 dnsmasq 配置防止配置误删除/无修改的方案，我看到后来所谓GitOps的思想。

就这么多吧，谢谢你看完我絮叨了这么些🙏！

## 参考

* [CI](https://zh.wikipedia.org/wiki/持續整合)
* [CD](https://zh.wikipedia.org/wiki/持續部署)
* [Devops](https://zh.wikipedia.org/wiki/DevOps)
* [Agile](https://zh.wikipedia.org/wiki/敏捷软件开发)
* [GitOps](https://xie.infoq.cn/article/c634551471f0338c53a6795e4)
* [Jenkins](https://www.jenkins.io)
* [Gitlab](https://about.gitlab.com)
* [Maven](https://maven.apache.org)
* [Harbor](https://goharbor.io)
* [Docker](https://www.docker.com)
* [Kubernetes](https://kubernetes.io)
* [Helm](https://helm.sh)
* [ArgoCD](https://argo-cd.readthedocs.io)
* [Tekton](https://www.tekton.com)

再多的参考资料都不如自己动手实践，共勉！

