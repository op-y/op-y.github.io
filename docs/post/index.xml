<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>自习室里的书架 on Qin的自习室</title>
    <link>https://op-y.github.io/post/</link>
    <description>Recent content in 自习室里的书架 on Qin的自习室</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Jan 2021 16:38:44 +0800</lastBuildDate><atom:link href="https://op-y.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于CI/CD</title>
      <link>https://op-y.github.io/cicd/</link>
      <pubDate>Fri, 08 Sep 2023 23:21:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/cicd/</guid>
      <description>写在前面 之前已经回顾过了
 流量接入与分发（Nginx/OpenResty/Kong/Traefik） 监控系统（Open Falcon/Prometheus/Thanos/VictoriaMetrics （提到过一点的）统一运维平台（其中包括基于Ansible开发的配置管理系统、对接第三方服务的DNS管理系统、WebUI等） （开了一个天坑）服务运行环境 Kubernetes  我们工作拼图中剩下的一大块就是CI/CD系统了，它串联起了项目的开发、测试、部署和运维管理完成过程，相信具有一定规模的公司里，CI/CD系统都是基础设施很重要的一环。本篇文章我们就来回顾一些我们在这方面做的一些工作。
简单讲讲CI/CD CI: 持续集成 CD：持续交付、持续部署
当我们谈论CI/CD的时候，我们谈论更多的是一种软件工程上的方法轮：软件开发团队之间如何高效协作完成产品的交付。
说的实际一点就是: 研发团队开发规范、团队间之间如何协调（代码的提交与集成）、测试团队如何高效完成测试、运维团队如何快速完成项目的部署、监控、维护和反馈。
以这个目标为核心，从瀑布模型到敏捷开发等等，出现了各种开发模式，以这些模型为基础又产生了各种工程实践，出现了系列的工具和系统，它们或多或少的解决了整个开发模型中某些环节的问题。
一个公司，无论处于什么阶段，都会建设自己的一套工具和系统，来实践自己开发模式。Baidu 的 Noah系统、Bytedance 的 ByteCycle（GitLab+SCM+TCE+&amp;hellip;) 等等。
CI/CD 实际上就是工程实践中的概念，是集成与部署的最终落地形式，即我们通过什么手段完成项目的集成与部署。
 Devops 最为常见的示意图 （图片来自网络）   Devops 的概念已经出现好几年了，很多公司都有自己的实践。有些通过自动化系统打通 RD/QA/OP 之间的环节提高效率；有些干脆模糊 RD/QA/OP 之间的界限，通过技术手段让研发人员能独立完成研发流程中的各个环节。不论是那种方式CI/CD一定是其中的环节之一。
 敏捷开发 Devops CI/CD 的关系（图片来自网络）   敏捷开发（Agile）、Devops、CI/CD 几个概念各不相同似乎有在说同一件事情，整个图比较能解释其中的异同，主要从关注点方面而言。
接下来要讲讲我们的做法了，当然主要是一些系统与故事。
我们最早的做法 我接触到的最早期的CI/CD系统入下图。
 最早期的CI/CD系统示意图   当时情况是大部分服务还在机房的物理机上，通过一些非常古老系统来部署运行（我们的业务是Java生态，当时使用了Tomcat、Jetty、Resin、Mule、AcitveMQ 这一类的Java Web Server、ESB、MQ等等，这些名词对于刚刚进入职场的同学而言都应该存在于历史书中。）
这个阶段我们的CI/CD非常原始，但是也有效。大概的过程是：
 一台物理机上部署了Jenkins，它通过插件能完成从GitLab拉取代码库并通过Maven进行构建生成Jar/War包； SRE同学通过人工方式（有些操作过于复杂没有交给Jenkins完成）去机器上执行一些部署脚本完成部署动作； 最后构建产生的Jar/War包在物理机上的各种系统中（Tomcat、Jetty、Resin、Mule 等）部署运行。  由于当时服务并不复杂，服务模块数量也不多，这种以Jenkins为中心辅之以自动化脚本的方式也算是能有效完成任务。
后续的转折发生在2017年，随着服务规模的不断扩大（陆陆续续扩大到有200+的服务模块），服务架构逐渐变得复杂（引入了Kong API Gateway、Docker、Kubernetes，后续还增加了阿里云环境形成了混合云架构），这套简易的CI/CD系统开始逐渐无法支撑SRE团队的业务了。最凸显的问题是，SRE人工执行部署脚本环节（这时候处理旧系统的部署的同时，还需针对Docker/Kubernetes做Image的构建和部署）变得异常耗费精力。我印象中工作最为繁重的时候，一周有一半的时间在处理服务发布。</description>
    </item>
    
    <item>
      <title>Traefik Ingress Controller</title>
      <link>https://op-y.github.io/traefik/</link>
      <pubDate>Thu, 07 Sep 2023 11:07:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/traefik/</guid>
      <description>引子 之前已经回顾了 OpenResty 和 Kong 有关的内容（当然 Nginx 相关内容默认是必知必会了），这两天隐隐约约觉得还差了一点什么东西，昨晚想起来了，是 Kubernetes 边缘路由（Edge Router），这里和IP层边缘路由协议 BGP 之类的概念做一些区别啊。
什么是边缘路由呢，直观地说，就是将来自 Kubernetes 集群外部请求代理到集群内部服务的反向代理（通常它会以DaemonSet/Deployment 的形式部署在 Kubernetes 集群中，通过 NodePort 方式将 Service 暴露到集群外部以接受请求）。它主要解决了服务在 Kubernetes 集群内部 IP 不固定会随时变化，需要动态更新路由信息的问题。
在 Kubernetes 中把进入流量的描述称为一种 Ingress 资源，边缘路由通常对 Ingress、Service、Endpoint 对象进行监视实时调整自己代理流量的行为，实现对进入请求的正确代理。所以这种边缘路由的服务也被称作 Ingress Controller。
当下Ingress Controller的实现有很多种，之前我们使用的是 Traefik2，所以接下来回顾一下 Traefik。
Traefik 是什么？ 其实在引言部分已经说的很清楚了。
Traefik 官方介绍
 Traefik 的 Logo 是一只交通管理员鼹鼠（非常的形象）   Traefik 官方给的架构图如下，很直观，都不用过多解释就能看懂它在干什么。
 Traefik 架构图   Traefik 安装&amp;amp;测试 我们的运行环境如下
 kuberntes 1.27.2  这里出现了 Kubernetes，关于 kubernetes（或者简称k8s）又是一个天坑，这里不打算多说。等有机会&amp;hellip;
Traefik 目前有 1.</description>
    </item>
    
    <item>
      <title>有关监控</title>
      <link>https://op-y.github.io/monitoring/</link>
      <pubDate>Wed, 06 Sep 2023 15:21:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/monitoring/</guid>
      <description>写在前面 写这篇文章不为其它，只为总结一下之前做过的一些关于监控系统的工作，或许不久的将来能用得上。
监控系统的演进 这里说的是曾经的一段工作经历中，公司监控系统的演进过程。包括了每个时期监控系统的建设背景，发展过程中遇到的问题，以及后续的演进方向。
古早时期（before 2017） 在2017年之前的监控系统：没有监控系统😂！
那个时期的业务，主要还是线下服务，线上服务刚刚起步，整个机房才20来台机器。我刚过去的时候完全没有监控系统。用户和业务线同学对故障容忍度出奇的高。印象比较深刻的一次，服务挂了半天后才恢复，没有人说啥，默默地恢复服务就好了。
随着2017年初，公司开始大力发展 SaaS 服务，这种情况开始改变了。某一天乐哥跟我说：监控系统了解过吗？有个国产的监控系统叫 Open Falcon，你去调研一下，我们部署一套试试！
2017-2019 说实话，之前我一直做业务运维，用过监控系统。但是监控系统长啥样？如何实现？我是完全没接触过的。这个时候我只能硬着头皮去学习Open Falcon了。
之后的一段时间只能说过得非常充实。我认真看了Open Falcon的官方介绍和文档（当时还是v0.1.x，后来升级到v0.2.x，之后就停止迭代了，现在这个站点都已经无法打开了&amp;hellip;）。跟着官方文档我一步一步搭建起了公司第一套监控系统。之后慢慢扩展到所有的机房。
 Open Falcon 架构   之后由于运维部开发统一的运维平台，我们有了将Open Falcon集成到平台的需求。这时候就需要对Open Falcon进行二次开发了。正是这个契机，促使我认真学习了Open Falcon的源码，对整个系统有了深入了解。在此基础上我们开发了一个定制版本的 api 模块。虽然这个模块现在已经完成了它的历史使命，但是依然在我的仓库了放着。
 falcon-api
 在使用Open Falcon过程中，顺带还开发了很多采集器（官方的采集器不少但是真的不够用）。
 falcon-plugins
 此外就是Open Falcon的 alarm 组件需要外部提供的 provider 才能真正实现告警消息的发送，为此我开发了一个功能比较丰富的 provider 程序。
 MixProvider
 日子就这样快乐地走到了2019年，Open Falcon使用过程中逐渐浮现出很多问题需要解决&amp;hellip;
2019-2021 2019年我们面临的情况是 Kubernetes 的流行以及在公司全面铺开。这个时候很多业务都在向 Kubernetes 中迁移。
对于那些在物理机/虚拟机上部署的中间件，Open Falcon 尚能支持；对于上 Kubernetes 的业务，Open Falcon 的能力就越来越捉襟见肘了。
由于业务程序的Pod会在集群中不停地迁移，业务程序只有两种选择：
 一种是将监控采集逻辑集成到代码里，主动向 agent 报告数据，这种方式对业务入侵太大； 另一种是集群中每个节点上 agent 能动态感知当前节点上正运行着哪些服务（其实本质上就是服务发现的问题），采集数据。 我们选择了前者，但显然做得不好&amp;hellip;  另外一个问题是，之前 Open Falcon 的部署，我们一直都是每个IDC独立部署一套。所有的 Open Falcon 系统不能统一管理，跨IDC的数据也不能聚合，存在一些使用局限。</description>
    </item>
    
    <item>
      <title>Kong 回顾</title>
      <link>https://op-y.github.io/kong/</link>
      <pubDate>Tue, 05 Sep 2023 22:52:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/kong/</guid>
      <description>引子 稍早之前对 OpenResty 进行了一个回顾，记得之前工作中，在接入网关升级改造中，使用 OpenResty 之后，我们又引入了 Kong API Gateway，所以也有必要对 Kong 简单的重新操作一下，做个记录。
Kong 是什么？ Kong Gateway 自己的官方的介绍
 Kong 的名字来自金刚 Logo就是这只猩猩   比较简单的理解：Kong Gateway 就是一个基于 OpenResty （不知道 OpenResty 是什么？去目录上找相关的文章）的 API Gateway （所以本质上还是 Nginx + Lua)。
 Kong Gateway 结构   至于 API Gateway 是什么？我个人比较认可它是一个贴近微服务架构的概念，它实现了一种Facade模式，统一各个服务的反向代理和请求管理，包括并不限于 认证、权限、限流、熔断、安全、监控、日志&amp;hellip; 从实现上说，它可以是个Nginx，也可以是一个OpenResty，还可以是 Kong、APISIX、BFE 等等。
这些年的变化 上一次自己部署 Kong 的时候，Kong 的版本还是 1.3，当时相比 0.1.x 版本已经有了很大的变化，是那种概念都不一样的非兼容变化。今天看了下官网，好家伙，都已经是 3.4.x 了，从 Changelogs 上看有了不小的调整（但是主要概念没有发生变化）。我看完后认为比较大的变化是：
 现在有4种部署模式  Konnect：这个1.3的时候还没注意到，是一个Kong Gateway的SaaS服务； Hybird: 这个是一个新的模式，将服务分成了 Control Panel和Data Panel，也是当下一种比较流行的做法； Tranditional：这个就是依赖DB部署的传统做法了，本质上时通过DB实现配置的中心化管理和分发； Db-less and declarative: 这个时无DB的声明式配置模式，单机部署相当的友好。   Traditional with-DB 模式下不再支持 Cassandra DB了（从3.</description>
    </item>
    
    <item>
      <title>OpenResty上手</title>
      <link>https://op-y.github.io/openresty/</link>
      <pubDate>Tue, 05 Sep 2023 14:42:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/openresty/</guid>
      <description>引子 没有想过要针对 Nginx、OpenResty、Kong 这类接入层中间件和系统专门写一些东西。最近因为DDDD的原因，需要回顾一下之前使用过的一些接入层开源系统以及做过的一些事情，所以简单的重新操作一下，做个记录。
OpenResty 是什么？ 官方首页上有比较官方的介绍
比较简单的理解：OpenResty 就是一个加强版的 Nginx，具体的加强之处就在于它给 Nginx 开发了一个 lua-nginx-module，这玩意儿在Nginx中直接集成了一个 Lua 解释器，让 Nginx 在处理请求的过程中可以执行内嵌的 Lua 脚本从而动态的实现一些 Nginx 原本很难做到的功能。
此外 OpenResty 官方还开发了很多可以直接使用的组件，用户可以直接拿来用；OpenResty 社区还有很多第三方的组件，方便用户开发自己的功能。
OpenResty 使用 安装 常见的操作系统都可以通过包管理工具直接安装对应的二进制版本。为了装逼有点参与感，我们还是源码编译安装。
我们的运行环境如下
 kernel: 5.10.0-15-amd64 系统发行版：Debian GNU/Linux 11  当前 OpenResty 的最新版本是 openresty-1.21.4.2，我们就选择整个版本，开始操作
cd ~ mkdir download &amp;amp;&amp;amp; cd download # 创建一个工作目录 wget https://openresty.org/download/openresty-1.21.4.2.tar.gz # 从官网下载源码 tar -zxvf openresty-1.21.4.2.tar.gz # 直接解压到当前目录  apt-get install libpcre3-dev libssl-dev perl make build-essential curl # 安装依赖  # 编译过程基本与nginx相同 # 我们指定了安装目录和一些需要安装的功能选项 cd openresty-1.</description>
    </item>
    
    <item>
      <title>Kubernetes的故事一: 前世今生</title>
      <link>https://op-y.github.io/k8s-history/</link>
      <pubDate>Sun, 20 Nov 2022 21:02:40 +0800</pubDate>
      
      <guid>https://op-y.github.io/k8s-history/</guid>
      <description>服务部署方式的变迁 当我们谈论部署服务的时候，我们在谈论什么？
TCE 或者一个更常被成为 Kubernetes 的东西！？ 写好代码、push 到代码库、SCM给我们一编译、然后去TCE平台上一键触发上线（当然更懒的我们会事先准备好一条ByteCycle流水线把这些动作串起来一键做了），顺利的话我们就可以喝着咖啡聊着天等待上线结果了。再顺利一点的话服务完全符合预期，收拾东西下班！
真就如此简单，从一开始？如果你是刚刚走出校园来到这个所谓的大厂里搬砖，那也许从一开始真就这么简单。而我，不禁回头看了一眼来时的路&amp;hellip;&amp;hellip;
 服务部署方式演进   传统部署 很久以前，那时还在另一个厂里搬砖&amp;hellip;同事们的代码是怎样部署的？新项目的话，先估计好需要的资源走流程申请，如果老板是个土豪（项目比较重要）就直接给你一堆物理机；如果老板比较抠门（项目太边缘）会说xxx的机器我看闲得很，你俩直接共用吧。不管是哪种情况，接下来聪明又懒惰的SRE同学会用早就准备好的自动化工具：编译代码生成二进制部署产物；将编译产物连同需要的其它配置文件远程复制到物理机上；执行部署脚本 停止旧服务-备份旧版本-部署新版本-启动新服务；当然SRE还会留一手如果出现问题，部署脚本还会快速回滚版本。
 优点：真是不要太简单，整个过程几个脚本或者一个简单的自动化工具就能搞定。 缺点：资源很难合理估计和利用，内存干到100%，CPU只用到10%；共同部署的应用真就成了命运共同体，一个占满资源所有的跟真倒霉；更苦笑不得的是那句，我在本地跑得好好的，咋部署在你这就不行了？  虚拟化部署 为了解决传统部署的问题，聪明的人们创造了一种全新的技术，当然今天我们听着老掉牙了，这就是虚拟化。不是说资源分配不合理吗？服务不好隔离吗？环境不统一吗？这么着，在物理机上做个虚拟层，一台机器秒变N台，资源按需分配，项目之间各管各的，还可以使用统一的OS Image让你不在有环境不统一的问题。至于部署上线，SRE还是可以偷偷懒，自动化管理VM的申请和回收，项目该如何上线还是如何上线。
 优点：资源可以按需申请了；项目之间不会相互产生影响，安全性隔离也做得很好；同一个项目环境也能统一了。 缺点：SRE怎么发现资源缩水了；RD发现程序运行效率咋就变低了；所有人都感觉用上了VM管理复杂了使用成本变高了。  容器化部署 为了解决虚拟层引入的新问题，聪明的人们翻起了故纸堆，惊奇的发现要做虚拟化不一定要OS on OS，有远见的开拓者们早就给我们准备好了两份礼物 Linux Control Group（CGroups）和 Linux Namespace (这两大技术先按下不表)，有这两样东西我们在kernel层就能按需分配资源、隔离进程了，没有了OS on OS这种臃肿结构，程序执行起来就更快了。SRE同学辛苦辛苦，部署上线就改成编译-生成容器-部署容器。
 优点：这下似乎资源利用率上来了，服务隔离了，环境似乎也统一了，运行速度够快； 缺点：似乎完美了&amp;hellip;  真的吗？
 虽然有了Cgroups 和 Namespace，一个隔离环境如何定义，大家会自成体系； Cgroups 和 Namespace 共用kernel，大家真的就隔离彻底了吗？ 以前物理机部署一个机器故障，换个机器重新部署就是；现在一个机器故障，上边可是几十成百的容器，怎么让他们能快速在其它机器上跑起来？ 从 SRE 效率上而言，传统部署时代的命题一个都不能少：怎么样做到横向扩展容器数量；如何跨云服务商、跨Linux操作系统发行版进行部署&amp;hellip;  Docker &amp;amp; Kubernetes 回望来时的路，我们已经知道答案，这一路上有两个名字是无法绕过的：Docker 和 Kubernetes！
在深入认识 Docker 和 Kubernetes 之前，先看一个故事。
一个故事: 容器编排之争 本故事来自 张磊 《深入剖析 Kubernetes》01 - 04，稍有改动和编辑，不能说非常相似只能说是一模一样，如有雷同纯属我抄他，分享给大家。</description>
    </item>
    
    <item>
      <title>一杯咖啡的诞生</title>
      <link>https://op-y.github.io/coffee-story/</link>
      <pubDate>Sun, 01 Aug 2021 23:46:45 +0800</pubDate>
      
      <guid>https://op-y.github.io/coffee-story/</guid>
      <description>引子 大家好，今天跟各位分享一个轻松一点的话题: 一杯咖啡是如何诞生的。
先简单声明：文中图片、视频和故事均来自网络和B站，请大家随意食用，我只是互联网的搬运工&amp;hellip;
早起一杯咖啡，精神一整天，已经是很多人的生活习惯；午后撸代码困了累了，来上一杯咖啡提神，休息一会儿，也已经是身边很多朋友调整节奏的方式；往前一两年，瑞幸咖啡用互联网营销方式“薅资本主义羊毛”，也让很多吃瓜群众看得不亦乐乎&amp;hellip;
咖啡如今早已是生活中寻常不过的事物。平常我们享用到的咖啡多数以下图中的形态呈现的：印着各种logo的纸杯或者马克杯中糅合着苦、酸与醇的提神醒脑饮品。这一杯究竟何来？我们开始关于咖啡的故事。
 各种品牌的咖啡   植物咖啡 咖啡豆大家在咖啡馆/咖啡店里多多少少见过。
“那咖啡豆又从何而来？”
“树上&amp;hellip;” 然后可能就说不上然后了。
   咖啡树
被子植物门|木兰纲|菊亚纲|茜草目|茜草科|咖啡族|咖啡属|咖啡种
多年生常绿灌木或小乔木，植株较矮小，在世界各地广泛种植，原产于非洲北部、中部亚热带地区，是一种多年生经济作物。
百度百科
   咖啡树   树上长得像樱桃的红色果实就是咖啡果，咖啡果从结果到成熟如下图。
 咖啡果成熟过程   一颗成熟的咖啡果由外向内解剖如下图。
 咖啡果解剖    咖啡果实结构示意图   细看一颗咖啡果实的结构，从外到内包括了：外果皮、果肉、果胶、羊皮、银皮、种子。咖啡果实经过加工去掉外果皮、果肉、果胶、羊皮之后得到的是咖啡生豆，生豆经过加工最后呈现的才是生活中我们常见的咖啡豆。
历史  牧羊人传说    6世纪的古阿比西尼亚（Abyssinia，现在的埃塞俄比亚），一名叫做卡尔迪的牧羊人，某天发现自己所放牧的羊群异常兴奋，他感到疑惑并追查后，发现原因似乎是羊群们吃下灌木丛所结的红色果实。卡拉迪某天到附近的修道院偶然提起这件事，修士们建议他不妨自己也试吃这种果实看看。卡尔迪和修士们一起试吃了果实之后，发现自己的身心变得大为畅快起来，精力充沛，被幸福感所包围，修士们大为吃惊，马上将果实带回修道院，分送给其他的修士。此后修士们在修行时能够一扫疲倦感，并且更加尽心尽力地修行。
  六世纪咖啡起源于非洲之角埃塞俄比亚。 十一世纪人们发现水煮咖啡比直接咀嚼更美味。 十三纪时埃塞俄比亚军队入侵也门，将咖啡带到了阿拉伯世界。因为伊斯兰教义禁止教徒饮酒，有的宗教界人士认为这种饮料刺激神经，违反教义，曾一度禁止并关闭咖啡店，但埃及苏丹认为咖啡不违反教义，因而解禁，咖啡饮料迅速在阿拉伯地区流行开来。 十七世纪之前咖啡的种植和生产一直为阿拉伯人所垄断。当时主要被使用在医学和宗教上，医生和僧侣们承认咖啡具有提神、醒脑、健胃、强身、止血等功效；十五世纪初开始有文献记载咖啡的使用方式，并且在此时期融入宗教仪式中，同时也出现在民间做为日常饮品。因伊斯兰教严禁饮酒，因此咖啡成为当时很重要的社交饮品。 1570年，土耳其军队围攻维也纳，失败撤退时，有人在土耳其军队的营房中发现一口袋黑色的种子，谁也不知道是什么东西。一个曾在土耳其生活过的波兰人，拿走了这袋咖啡，在维也纳开了第一家咖啡店。16世纪末，咖啡以 “伊斯兰酒” 的名义通过意大利开始大规模传入欧洲。相传当时一些天主教人士认为咖啡是 “魔鬼饮料”，怂恿当时的教皇克莱门八世禁止这种饮料，但教皇品尝后认为可以饮用，并且祝福了咖啡，因此咖啡在欧洲逐步普及。 起初咖啡在欧洲价格不菲，只有贵族才能饮用咖啡，咖啡甚至被称为“黑色金子”。直到1690年，一位荷兰船长航行到也门，得到几棵咖啡苗，在印度尼西亚种植成功。 1727年荷属圭亚那的一位外交官的妻子，将几粒咖啡种子送给一位在巴西的西班牙人，他在巴西试种取得很好的效果。巴西的气候非常适宜咖啡生长，从此咖啡在南美洲迅速蔓延。因大量生产而价格下降的咖啡开始成为欧洲人的重要饮料。  至此世界咖啡种植的格局基本形成。非洲、亚洲、美洲的热带地区是咖啡的主要种植地。下图大致展示了主要的咖啡产地，详情请参考文后参考文献《世界咖啡地图》。
 世界咖啡带    非洲  布隆迪 埃塞俄比亚  耶加雪菲（Yirgacheffe） 哈拉（Harrar）   肯尼亚  肯尼亚AA（Kenya AA）   马拉维 卢旺达  机无（kivu）   坦桑尼亚  乞力马扎罗（Kilimanjaro）   赞比亚   亚太  印度 印度尼西亚  爪哇（Java） 曼特宁（Mandheling）   巴布亚新几内亚 越南 也门 美国夏威夷  柯那（Kona）     美洲  玻利维亚 巴西  山多士（Santos） 摩吉安纳（Mogiana）   哥伦比亚 哥斯达黎加 古巴 多米尼加 厄瓜多尔 萨尔瓦多 危地马拉  安提瓜（Antigua）   洪都拉斯 牙买加  蓝山（Blue Mountain）   墨西哥 尼加拉瓜 巴拿马 秘鲁 委内瑞拉    品种：阿拉比卡 vs 罗布斯塔 从埃塞俄比亚走出来的原始品种咖啡，在传播过程中繁衍出了庞大的品种家族，其中还包括了很多实验室人造的品种。大体来说今天我们能喝到的咖啡，多数是下图咖啡品种树中，桔色（阿拉比卡/蒂比卡）和 紫色（罗布斯塔）这两支。很多咖啡广告里都会强调自己用的是阿拉比卡咖啡豆，为什么会这样？我们来探究一下。</description>
    </item>
    
    <item>
      <title>如何使用Hugo生成文章内容</title>
      <link>https://op-y.github.io/how_to_use_this_blog/</link>
      <pubDate>Wed, 13 Jan 2021 16:40:44 +0800</pubDate>
      
      <guid>https://op-y.github.io/how_to_use_this_blog/</guid>
      <description>前置要求 首先学会使用Git。可以直接阅读《Pro Git》的在线版。
然后知道一些基本的Markdown语法，文章内容使用Markdown+一些主题特定的语法进行编辑。熟悉Markdown语法能编辑出样式精美的文章。
使用工具 文章是使用 Hugo 生成的，目前使用的主题是 Cupper。需要知道Hugo的基本概念以及 hugo、 hugo new、hugo server 这些基本操作。内容编辑还可以参考主题目录下的示例文件。
 Cupper 主题提供的特殊样式
 使用方式  将项目clone到本地 从master/main分支checkout出开发分支，如果是已经存在开发分支则是merge一下master/main分支最近的提交 git submodule init 初始化子项目（主题，初次使用时操作） git submodule update 拉取子项目（主题，可选） hugo new 创建新的文章 hugo server 用本地server调试文章页面 hugo 生成站点静态文件 将你开发分支新内容merge回master/main分支 在master/main分支上生成新静态文件 将内容push到Github  </description>
    </item>
    
  </channel>
</rss>
