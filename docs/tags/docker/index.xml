<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker on Qin的自习室</title>
    <link>https://op-y.github.io/tags/docker/</link>
    <description>Recent content in docker on Qin的自习室</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Sep 2023 23:21:00 +0800</lastBuildDate><atom:link href="https://op-y.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>关于CI/CD</title>
      <link>https://op-y.github.io/cicd/</link>
      <pubDate>Fri, 08 Sep 2023 23:21:00 +0800</pubDate>
      
      <guid>https://op-y.github.io/cicd/</guid>
      <description>写在前面 之前已经回顾过了
 流量接入与分发（Nginx/OpenResty/Kong/Traefik） 监控系统（Open Falcon/Prometheus/Thanos/VictoriaMetrics （提到过一点的）统一运维平台（其中包括基于Ansible开发的配置管理系统、对接第三方服务的DNS管理系统、WebUI等） （开了一个天坑）服务运行环境 Kubernetes  我们工作拼图中剩下的一大块就是CI/CD系统了，它串联起了项目的开发、测试、部署和运维管理完成过程，相信具有一定规模的公司里，CI/CD系统都是基础设施很重要的一环。本篇文章我们就来回顾一些我们在这方面做的一些工作。
简单讲讲CI/CD CI: 持续集成 CD：持续交付、持续部署
当我们谈论CI/CD的时候，我们谈论更多的是一种软件工程上的方法轮：软件开发团队之间如何高效协作完成产品的交付。
说的实际一点就是: 研发团队开发规范、团队间之间如何协调（代码的提交与集成）、测试团队如何高效完成测试、运维团队如何快速完成项目的部署、监控、维护和反馈。
以这个目标为核心，从瀑布模型到敏捷开发等等，出现了各种开发模式，以这些模型为基础又产生了各种工程实践，出现了系列的工具和系统，它们或多或少的解决了整个开发模型中某些环节的问题。
一个公司，无论处于什么阶段，都会建设自己的一套工具和系统，来实践自己开发模式。Baidu 的 Noah系统、Bytedance 的 ByteCycle（GitLab+SCM+TCE+&amp;hellip;) 等等。
CI/CD 实际上就是工程实践中的概念，是集成与部署的最终落地形式，即我们通过什么手段完成项目的集成与部署。
 Devops 最为常见的示意图 （图片来自网络）   Devops 的概念已经出现好几年了，很多公司都有自己的实践。有些通过自动化系统打通 RD/QA/OP 之间的环节提高效率；有些干脆模糊 RD/QA/OP 之间的界限，通过技术手段让研发人员能独立完成研发流程中的各个环节。不论是那种方式CI/CD一定是其中的环节之一。
 敏捷开发 Devops CI/CD 的关系（图片来自网络）   敏捷开发（Agile）、Devops、CI/CD 几个概念各不相同似乎有在说同一件事情，整个图比较能解释其中的异同，主要从关注点方面而言。
接下来要讲讲我们的做法了，当然主要是一些系统与故事。
我们最早的做法 我接触到的最早期的CI/CD系统入下图。
 最早期的CI/CD系统示意图   当时情况是大部分服务还在机房的物理机上，通过一些非常古老系统来部署运行（我们的业务是Java生态，当时使用了Tomcat、Jetty、Resin、Mule、AcitveMQ 这一类的Java Web Server、ESB、MQ等等，这些名词对于刚刚进入职场的同学而言都应该存在于历史书中。）
这个阶段我们的CI/CD非常原始，但是也有效。大概的过程是：
 一台物理机上部署了Jenkins，它通过插件能完成从GitLab拉取代码库并通过Maven进行构建生成Jar/War包； SRE同学通过人工方式（有些操作过于复杂没有交给Jenkins完成）去机器上执行一些部署脚本完成部署动作； 最后构建产生的Jar/War包在物理机上的各种系统中（Tomcat、Jetty、Resin、Mule 等）部署运行。  由于当时服务并不复杂，服务模块数量也不多，这种以Jenkins为中心辅之以自动化脚本的方式也算是能有效完成任务。
后续的转折发生在2017年，随着服务规模的不断扩大（陆陆续续扩大到有200+的服务模块），服务架构逐渐变得复杂（引入了Kong API Gateway、Docker、Kubernetes，后续还增加了阿里云环境形成了混合云架构），这套简易的CI/CD系统开始逐渐无法支撑SRE团队的业务了。最凸显的问题是，SRE人工执行部署脚本环节（这时候处理旧系统的部署的同时，还需针对Docker/Kubernetes做Image的构建和部署）变得异常耗费精力。我印象中工作最为繁重的时候，一周有一半的时间在处理服务发布。</description>
    </item>
    
    <item>
      <title>Kubernetes的故事一: 前世今生</title>
      <link>https://op-y.github.io/k8s-history/</link>
      <pubDate>Sun, 20 Nov 2022 21:02:40 +0800</pubDate>
      
      <guid>https://op-y.github.io/k8s-history/</guid>
      <description>服务部署方式的变迁 当我们谈论部署服务的时候，我们在谈论什么？
TCE 或者一个更常被成为 Kubernetes 的东西！？ 写好代码、push 到代码库、SCM给我们一编译、然后去TCE平台上一键触发上线（当然更懒的我们会事先准备好一条ByteCycle流水线把这些动作串起来一键做了），顺利的话我们就可以喝着咖啡聊着天等待上线结果了。再顺利一点的话服务完全符合预期，收拾东西下班！
真就如此简单，从一开始？如果你是刚刚走出校园来到这个所谓的大厂里搬砖，那也许从一开始真就这么简单。而我，不禁回头看了一眼来时的路&amp;hellip;&amp;hellip;
 服务部署方式演进   传统部署 很久以前，那时还在另一个厂里搬砖&amp;hellip;同事们的代码是怎样部署的？新项目的话，先估计好需要的资源走流程申请，如果老板是个土豪（项目比较重要）就直接给你一堆物理机；如果老板比较抠门（项目太边缘）会说xxx的机器我看闲得很，你俩直接共用吧。不管是哪种情况，接下来聪明又懒惰的SRE同学会用早就准备好的自动化工具：编译代码生成二进制部署产物；将编译产物连同需要的其它配置文件远程复制到物理机上；执行部署脚本 停止旧服务-备份旧版本-部署新版本-启动新服务；当然SRE还会留一手如果出现问题，部署脚本还会快速回滚版本。
 优点：真是不要太简单，整个过程几个脚本或者一个简单的自动化工具就能搞定。 缺点：资源很难合理估计和利用，内存干到100%，CPU只用到10%；共同部署的应用真就成了命运共同体，一个占满资源所有的跟真倒霉；更苦笑不得的是那句，我在本地跑得好好的，咋部署在你这就不行了？  虚拟化部署 为了解决传统部署的问题，聪明的人们创造了一种全新的技术，当然今天我们听着老掉牙了，这就是虚拟化。不是说资源分配不合理吗？服务不好隔离吗？环境不统一吗？这么着，在物理机上做个虚拟层，一台机器秒变N台，资源按需分配，项目之间各管各的，还可以使用统一的OS Image让你不在有环境不统一的问题。至于部署上线，SRE还是可以偷偷懒，自动化管理VM的申请和回收，项目该如何上线还是如何上线。
 优点：资源可以按需申请了；项目之间不会相互产生影响，安全性隔离也做得很好；同一个项目环境也能统一了。 缺点：SRE怎么发现资源缩水了；RD发现程序运行效率咋就变低了；所有人都感觉用上了VM管理复杂了使用成本变高了。  容器化部署 为了解决虚拟层引入的新问题，聪明的人们翻起了故纸堆，惊奇的发现要做虚拟化不一定要OS on OS，有远见的开拓者们早就给我们准备好了两份礼物 Linux Control Group（CGroups）和 Linux Namespace (这两大技术先按下不表)，有这两样东西我们在kernel层就能按需分配资源、隔离进程了，没有了OS on OS这种臃肿结构，程序执行起来就更快了。SRE同学辛苦辛苦，部署上线就改成编译-生成容器-部署容器。
 优点：这下似乎资源利用率上来了，服务隔离了，环境似乎也统一了，运行速度够快； 缺点：似乎完美了&amp;hellip;  真的吗？
 虽然有了Cgroups 和 Namespace，一个隔离环境如何定义，大家会自成体系； Cgroups 和 Namespace 共用kernel，大家真的就隔离彻底了吗？ 以前物理机部署一个机器故障，换个机器重新部署就是；现在一个机器故障，上边可是几十成百的容器，怎么让他们能快速在其它机器上跑起来？ 从 SRE 效率上而言，传统部署时代的命题一个都不能少：怎么样做到横向扩展容器数量；如何跨云服务商、跨Linux操作系统发行版进行部署&amp;hellip;  Docker &amp;amp; Kubernetes 回望来时的路，我们已经知道答案，这一路上有两个名字是无法绕过的：Docker 和 Kubernetes！
在深入认识 Docker 和 Kubernetes 之前，先看一个故事。
一个故事: 容器编排之争 本故事来自 张磊 《深入剖析 Kubernetes》01 - 04，稍有改动和编辑，不能说非常相似只能说是一模一样，如有雷同纯属我抄他，分享给大家。</description>
    </item>
    
  </channel>
</rss>
